version: '3.5'
services:
#  mssql-db:
#    image: mysql/mysql-server
#    restart: always
#    environment:
#      ACCEPT_EULA: Y
#      SA_PASSWORD: password!1
#    ports:
#    - 1430:1433
#    container_name: bigdata-cluster-mssql-db
#    networks:
#      - bigdata-cluster-network
#  mssql-db-data-load :
#    build:  ./mssql-db-data-load/
#    command: sh -c './wait-for-it.sh mssql-db:1433 --timeout=0 --strict -- sleep 10 && ./run-data-load.sh'
#    container_name: bigdata-cluster-mssql-db-data-load
#    depends_on:
#      - "mssql-db"
#    networks:
#      - bigdata-cluster-network
#  bigdata-cluster-hadoop:
#    build: ./bigdata-cluster-hadoop/
#    restart: always
#    container_name: bigdata-cluster-hadoop
#    command: sh 'start-hadoop-ecosystem.sh'
#    depends_on:
#      - mssql-db
#    ports:
#    - "22:22"
#    - "9000:9000"
#    - "8088:8088"
#    - "9870:9870"
#    - "10000:10000"
#    - "9999:9999"
#    - "9083:9083"
#    - "10500:10500"
#    networks:
#      - bigdata-cluster-network
#  bigdata-cluster-mpp:
#    build: ./bigdata-cluster-mpp/
#    restart: always
#    container_name: bigdata-cluster-mpp
#    command: sh 'start-mpp-ecosystem.sh'
#    ports:
#    - "23:22"
#    - "9092:9092"
#    networks:
#      - bigdata-cluster-network
  mock-api:
    build: mock-api
    container_name: mock-air-api
    ports:
    - "5000:1337"
  nifi:
    build: nifi
    container_name: bigdata-nifi
    ports:
    - "8443:8443" # https://docs.docker.com/compose/compose-file/compose-file-v3/#ports Pamietajmy o cudzyslowach!
    volumes:
    - ./nifi/nifi-mount:/home/user

  # https://github.com/bitnami/bitnami-docker-kafka/issues/159
  kafka:
    image: bitnami/kafka:3.1.0
    container_name: bigdata-kafka
    command:
      - 'sh'
      - '-c'
      - '/opt/bitnami/scripts/kafka/setup.sh && kafka-storage.sh format --config "$${KAFKA_CONF_FILE}" --cluster-id "lkorDA4qT6W1K_dk0LHvtg" --ignore-formatted  && /opt/bitnami/scripts/kafka/run.sh' # Kraft specific initialise
    environment:
      - ALLOW_PLAINTEXT_LISTENER=yes
      # Start Kraft Setup (Kafka as Controller - no Zookeeper)
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_BROKER_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@127.0.0.1:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LOG_DIRS=/tmp/logs
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      # End Kraft Specific Setup
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092
    ports:
      - "9092:9092"

  spark:
    image: docker.io/bitnami/spark:3
    container_name: spark-straming
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8888:8080'


  # bigdata-cluster-elasticsearch:
  #   build: ./bigdata-cluster-elasticsearch/
  #   restart: always
  #   container_name: bigdata-cluster-elasticsearch
  #   command: sh 'start-elasticsearch.sh'
  #   ports:
  #   - "25:22"
  #   - "9200:9200"
  #   networks:
  #     - bigdata-cluster-network
